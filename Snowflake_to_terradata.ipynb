{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10527310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryptography.hazmat.backends import default_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e39492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryptography.hazmat.primitives import serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b4392f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
      "Collecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=21d788cdf40cc2a3eb6a61c751377b2bb31014b03f4a7661388b1b47ec9f6e09\n",
      "  Stored in directory: c:\\users\\lohit\\appdata\\local\\pip\\cache\\wheels\\27\\3e\\a7\\888155c6a7f230b13a394f4999b90fdfaed00596c68d3de307\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd65124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d35d5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.context import SparkContext\n",
    "import re\n",
    "import sys\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cab648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"yarn\")\\\n",
    "                .appName('TerraLoad') \\\n",
    "                .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488abd53",
   "metadata": {},
   "source": [
    "We are trying to connect to snowflake and fetch the data available in snowflake. \n",
    "\n",
    "We have many authentication techniques but we are using private/public key authentication. \n",
    "\n",
    "before trying to authenticate here, we need to add public key to snowflake as authorized user(ssh_public_key). \n",
    "\n",
    "This below code is to take the private key file and password, deserialize it for further for consumption\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f7df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"path_to_rsa_key.p8\",\"rb\") as key_file:\n",
    "    p_key = serialization.load_pem_private_key(\n",
    "    key_file.read(),\n",
    "    password=\"private key file password\".encode(),\n",
    "    backend=default_backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkb = p_key.private_bytes(encoding = serialization.Encoding.PEM,format=serialization.PrivateFormat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a865a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pkb = p_key.private_bytes(encoding-serialization.Encoding.PEM,format=serialization.PrivateFormat.PKCS8, \n",
    "                          encryption_algorithm=serialization.NoEncryption())\n",
    "\n",
    "pkb = pkb.decode(\"UTF-8\")\n",
    "\n",
    "pkb = re.sub(\"-*(BEGIN|END) PRIVATE KEY-*\\n\", \"\" ,pkb). replace(\"\\n\", \"\")\n",
    "\n",
    "sfOptions ={\n",
    "\"sfURL\" : \"https://<account>.east-us-2.azure.snowflakecomputing.com\",\n",
    "\"sfAccount\" : \"<account>\",\n",
    "\"'sfUser\" :\"lohithkumar036@gmail.com\"\n",
    "\"sfDatabase\": \"PROSPECTIVE_DB\",\n",
    "\"sfSchema\" : \"FOUNDATION\",\n",
    "\"sfWarehouse\" : \"PROSPECTIVE_ANALYTICS_WH\",\n",
    "\"sfRole\" : \"AR_ROLE_LOHITHKUMAR_ROLE\",\n",
    "\"pem_private_key\":pkb\n",
    "}\n",
    "\n",
    "snowflake_source_name = \"net.snowflake.spark.snowflake\"\n",
    "df = spark.read.format(snowflake_source_name)\\\n",
    "        .options(**sfOptions)\\\n",
    "        .option(\"query\",\"\"\"\n",
    "        select * from table <your query here>\n",
    "        \"\"\").load()\n",
    "\n",
    "    \n",
    "# if you want to save data as parquet file in hdfs location \n",
    "df.repartition(1).write.mode (\"overwrite\").parquet(\"<location where you wanted to save as parquet>\")\n",
    "\n",
    "#if you want to save data directly to terradata\n",
    "df.write. format(\"jdbc\")\\\n",
    "    .option (\"driver\", \"com.teradata.jdbc.TeraDriver\") \\\n",
    "    .option (\"url\",\"jdbc:teradata://<account>/DATABASE=***,DBS_PORT=***, LOGMECH=LDAP, TYPE=FASTEXPORT,TMODE=TERA\")\\\n",
    "    .option (\"dbtable\", \"<table name>\")\\ \n",
    "    .option (\"user\", \"<username>\")\\ \n",
    "    .option (\"password\", \"<password>\")\\ \n",
    "    .mode (\"append\" )\\\n",
    "    .save ()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
